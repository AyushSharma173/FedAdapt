<!-- ````markdown -->
# Self-Adaptive Personalization

> Dynamically blend each clientâ€™s local and global model per round to mitigate non-IID drift.

---

## ğŸ¯ Motivation

Federated Learning on non-IID data can bias the global model toward dominant shards.  
**Self-Adaptive Personalization** lets each client maintain both:
- a **local** model updated on its own data  
- the **global** model received from the server  

Each round, the client automatically adjusts a mixing weight **Î±âˆˆ[0,1]** to favor whichever model currently performs better on held-out data.

---

## âš™ï¸ How It Works

1. **Evaluate** both models on the clientâ€™s validation split:
   ```python
   local_acc  = self.evaluate_model(self.local_weights)
   global_acc = self.evaluate_model(self.global_weights)
<!-- ```` -->

2. **Adjust** the mixing weight Î± by threshold Ï„ and step Î”:

   ```python
   if local_acc - global_acc > Ï„:
       self.alpha = min(self.alpha + Î”, 1.0)
   elif global_acc - local_acc > Ï„:
       self.alpha = max(self.alpha - Î”, 0.0)
   ```
3. **Mix** the weights before training:

   ```python
   mixed = [
     self.alpha * lw + (1 - self.alpha) * gw
     for lw, gw in zip(self.local_weights, self.global_weights)
   ]
   self.model_adapter.set_weights(mixed, is_aggregator=False)
   ```
4. **Train** with standard local SGD on `mixed`, then store updated weights in `self.local_weights`.

---

## ğŸ›  Configuration

Expose two hyperparameters in both **CLI/REST** and the dashboard UI:

| Flag / Form Field   | Description                     | Default |
| ------------------- | ------------------------------- | :-----: |
| `--alpha_threshold` | Ï„: min accuracy gap to adjust Î± |   0.02  |
| `--alpha_step`      | Î”: step size for Î± adjustments  |   0.10  |

### Dashboard UI Snippet

<details>
<summary><strong>Self-Adaptive Personalization (settings)</strong></summary>

```jsx
<fieldset>
  <legend>Self-Adaptive Personalization</legend>
  <label>
    Î±-threshold (Ï„):
    <input
      name="alpha_threshold"
      type="number" step="0.005" min="0" max="1"
      value={form.alpha_threshold}
      onChange={handleChange}
    />
  </label>
  <label>
    Î±-step (Î”):
    <input
      name="alpha_step"
      type="number" step="0.01" min="0" max="1"
      value={form.alpha_step}
      onChange={handleChange}
    />
  </label>
</fieldset>
```

</details>

![Self-Adaptive Settings](assets/self-adaptive-settings.png)

---

## ğŸ“Š Visualizing Personalization

Your dashboard can chart each clientâ€™s Î± over rounds:

![Alpha Trajectories](assets/alpha-trajectories.png)

> **Tip**: Higher Ï„/Î” makes Î± more conservative (slower to adapt).
> To push for stronger personalization, lower Ï„ or increase Î”.

---

## ğŸ”§ Defaults & Tuning Tips

* **Defaults**:

  * Ï„ = **0.02**, Î” = **0.10** (empirically stable for CIFAR-10, Dirichlet Î±=0.5)
* **More personalization**:

  * **Lower Ï„** â†’ more frequent Î± updates
  * **Increase Î”** â†’ larger jumps in Î± per round
* **Less personalization**: do the opposite (higher Ï„, smaller Î”)

---

## ğŸ— Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      gRPC      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI / Uvicorn   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FedScale Aggregatorâ”‚
â”‚ (dashboard-backend)  â”‚                â”‚    & Executor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â–²
         â”‚ HTTP / SSE                        â”‚ Evaluate + Mix
         â–¼                                    â”‚ Adjust Î±
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚ Train on mixed weights
â”‚  React + Vite        â”‚                      â”‚
â”‚ (dashboard-frontend) â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚   Local Client     â”‚
                                      â”‚(self-adaptive logic)â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Dashboard/API**: user sets Ï„, Î”
2. **Executor**: each round â†’ evaluate local vs global â†’ adjust Î± â†’ mix â†’ train
3. **Aggregator**: standard FL orchestration + streams metrics

---

*Read more about the math and implementation in [the research paper](http://paper.fedadapt.com.s3-website.us-east-2.amazonaws.com/) or see the code in*
`fedscale/cloud/execution/executor.py â†’ Executor.Train()`

```
```
