<!-- ````markdown -->
# Heterogeneity-Aware Co-Optimization

> Dynamically tailor each clientâ€™s work (local steps & compression) to minimize per-round runtime.

---

## ğŸ¯ Motivation

Real-world devices vary wildly in compute speed, network bandwidth, and availability.  
A fixed â€œone-size-fits-allâ€ choice of local steps or gradient compression can:  
- Slow down overall wall-clock convergence (stragglers)  
- Waste bandwidth on fast clients  
- Under-utilize powerful devices  

**Heterogeneity-Aware Co-Optimization** automatically picks, _per client per round_, the best:
- **ğ‘˜** = number of local SGD steps  
- **ğ‘** = compression ratio on the model update  

to minimize  
\[
T_i(k,c) = k \cdot t_{\text{comp},i} \;+\; c \cdot B \cdot t_{\text{comm},i}
\]  
where each clientâ€™s compute & comm timings \((t_{\text{comp},i},t_{\text{comm},i})\) are profiled in-situ, and \(B\) is the model-update size.

---

## âš™ï¸ How It Works

Inside the **Aggregator**â€™s `round_completion_handler()`:

```python
# profile: perâ€client compute & comm times
profiles = {
    client: {
      "t_step": comp_i,
      "t_comm": comm_i / model_bits
    }
    for client, (comp_i, comm_i) in profiled_times.items()
}

# grid search over k âˆˆ {1, default_steps}, c âˆˆ {1.0, 0.5, 0.25}
new_conf = {}
for client, prof in profiles.items():
    best = None
    for k in [1, args.local_steps]:
      for c in [1.0, 0.5, 0.25]:
        T = k * prof["t_step"] + c * model_bits * prof["t_comm"]
        if best is None or T < best[0]:
            best = (T, k, c)
    new_conf[client] = Namespace(
      local_steps=best[1],
      compression=best[2],
      batch_size=args.batch_size,
      # â€¦other fieldsâ€¦
    )

self.client_conf = new_conf
<!-- ```` -->

Each client then sees its tuned `(k, c)` on the next `CLIENT_TRAIN` event.

---

## ğŸ›  Configuration

Expose three fields in your CLI/REST and dashboard:

| Flag / Form Field | Description | Default |
| ----------------- | ----------- | :-----: |
| `--optimize_for`  | Strategy:   |         |

* **Fastest Training**
* **Balanced**
* **Best Accuracy**            | `Balanced` |
  \| `--compression_limit`| Maximum allowed compression ratio `[0.1â€“1.0]`  | `1.0`      |
  \| `--auto_tune`        | Enable/disable automatic per-client tuning     | `True`     |

### Dashboard UI Snippet

<details>
<summary><strong>Heterogeneity-Aware Optimization (settings)</strong></summary>

```jsx
<fieldset>
  <legend>Heterogeneity-Aware Optimization</legend>

  <label>
    Optimize For:
    <select
      name="optimize_for"
      value={form.optimize_for}
      onChange={handleChange}
    >
      <option>Fastest Training</option>
      <option>Balanced</option>
      <option>Best Accuracy</option>
    </select>
  </label>

  <label>
    Compression Limit (câ‚˜â‚â‚“):
    <input
      name="compression_limit"
      type="number" step="0.1" min="0.1" max="1.0"
      value={form.compression_limit}
      onChange={handleChange}
    />
  </label>

  <label>
    Auto Tune:
    <select
      name="auto_tune"
      value={form.auto_tune.toString()}
      onChange={e => setForm(f => ({ â€¦f, auto_tune: e.target.value === 'true' }))}
    >
      <option value="true">Enabled</option>
      <option value="false">Disabled</option>
    </select>
  </label>
</fieldset>
```

</details>

---

## ğŸ“Š Visualizing Co-Optimization

On the dashboard you can plot per-client choices each round:

* **Local Steps (k)** vs **Round**
* **Compression Ratio (c)** vs **Round**
* **Bandwidth Saved** (baseline âˆ’ actual)

![Co-Opt Metrics](assets/coopt-metrics.png)

> **Tip**:
>
> * Lower `compression_limit` forces more aggressive sparsification.
> * Disable `auto_tune` to use a single global `(k,c)` fallback.

---

## ğŸ”§ Defaults & Tuning Tips

* **Balanced**: trades off compute vs. comm.
* **Fastest Training**: biases toward more local steps on fast clients.
* **Best Accuracy**: biases toward lower compression.
* If you care more about accuracy than speed, pick **Best Accuracy** and set `compression_limit` near 1.0.

---

## ğŸ— Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      gRPC      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI / Uvicorn   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FedScale Aggregatorâ”‚
â”‚ (dashboard-backend)  â”‚                â”‚    & Executor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â–²
         â”‚ HTTP / SSE                        â”‚ Gridâ€search (k,c)
         â–¼                                    â”‚ perâ€client
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  React + Vite        â”‚                      â–¼
â”‚ (dashboard-frontend) â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   Co-Optimizer     â”‚
                                        â”‚ (in Aggregator)    â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Read more in [the research paper](https://arxiv.org/abs/â€¦) or see the code in*
`fedscale/cloud/aggregation/aggregator.py â†’ Aggregator.round_completion_handler()`

````
